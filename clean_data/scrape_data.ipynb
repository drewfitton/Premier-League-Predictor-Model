{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping for League Results and Odds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "import requests\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Working Directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = os.path.dirname(os.path.abspath('__file__'))\n",
    "DATA_DIR = os.path.join(BASE_DIR, 'data_out')\n",
    "# Replace with the path to your chromedriver\n",
    "CHROMEDRIVER_DIR = os.path.join(BASE_DIR, 'chromedriver-mac-x64','chromedriver')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Postgres Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_user = 'db_user'    # Replace with your PostgreSQL username\n",
    "db_password = 'db_password'  # Replace with your PostgreSQL password\n",
    "db_host = 'localhost'      # Replace with your PostgreSQL host (e.g., localhost or IP)\n",
    "db_port = '5432'           # PostgreSQL port (default is 5432)\n",
    "db_name = 'db_name'  # Replace with your PostgreSQL db name  \n",
    "\n",
    "connection_string = f\"postgresql+psycopg2://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}\"\n",
    "\n",
    "# Create the SQLAlchemy engine\n",
    "engine = create_engine(connection_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table queried successfully.\n",
      "Connection closed.\n"
     ]
    }
   ],
   "source": [
    "#Create query\n",
    "query = \"SELECT * FROM raw_match_data\"\n",
    "# Read the dfFrame from a PostgreSQL table\n",
    "try:\n",
    "    df = pd.read_sql(query, engine)\n",
    "    print(\"Table queried successfully.\")\n",
    "    \n",
    "finally:\n",
    "    # Ensure connection is closed\n",
    "    engine.dispose()\n",
    "    print(\"Connection closed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(df) > 0:\n",
    "    recent_date = datetime.strptime(df['date'][0], \"%d %b %Y\")\n",
    "else:\n",
    "    recent_date = datetime.strptime(\"10 Jan 2000\", \"%d %b %Y\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-01-25 00:00:00\n"
     ]
    }
   ],
   "source": [
    "print(recent_date)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Season URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getURLs():\n",
    "    curr_year = datetime.now().year - 1\n",
    "\n",
    "    seasons = list(reversed(range(curr_year - 20, curr_year)))\n",
    "\n",
    "    root_url = 'https://www.oddsportal.com/soccer/england/premier-league'\n",
    "    results_path = '/results/'\n",
    "    results_url = root_url + results_path\n",
    "\n",
    "    #Get URLs for results pages for every season\n",
    "    seasons_url = [root_url + '-' + str(season) + '-' + str(season + 1) + results_path for season in seasons]\n",
    "\n",
    "    #complete url list to be scraped\n",
    "    return [results_url] + seasons_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize scroll function to click page links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def scroll_to_element_and_click(driver, element):\n",
    "    # Scroll the element into view\n",
    "    driver.execute_script(\"arguments[0].scrollIntoView(true);\", element)\n",
    "    # Add a small delay if needed for any animation or page shift\n",
    "    time.sleep(3)\n",
    "    try:\n",
    "        button = driver.find_element(By.ID, \"onetrust-accept-btn-handler\")  # Replace with the actual button ID\n",
    "        button.click()\n",
    "        time.sleep(1)\n",
    "    except NoSuchElementException:\n",
    "        print(\"No cookies!\")\n",
    "    # Click the element after scrolling\n",
    "    element.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WebScrape OddsPortal for results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "service = Service(executable_path=CHROMEDRIVER_DIR)\n",
    "options = webdriver.ChromeOptions()\n",
    "driver = webdriver.Chrome(service=service, options=options)\n",
    "driver.implicitly_wait(2)\n",
    "\n",
    "new_df = pd.DataFrame()\n",
    "unique_matches = set()  # To track unique matches\n",
    "breakAll = False\n",
    "\n",
    "all_urls = getURLs()\n",
    "# Iterate over urls for seasons\n",
    "for url in all_urls:\n",
    "    driver.get(url)\n",
    "\n",
    "    #Get every page for current URL\n",
    "    WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'pagination')))\n",
    "    pagination_container = driver.find_element(By.CLASS_NAME, 'pagination')\n",
    "    pagination_links = pagination_container.find_elements(By.CLASS_NAME, 'pagination-link')\n",
    "    if len(pagination_links) > 1:\n",
    "        pagination_links = pagination_links[:-1]\n",
    "\n",
    "    previous_page = None\n",
    "\n",
    "    for link in pagination_links:\n",
    "        current_page = link.get_attribute(\"data-number\")\n",
    "        \n",
    "        # Click the pagination link and wait for the page to load\n",
    "        if previous_page:\n",
    "            #print(f\"Navigating from page {previous_page} to page {current_page}\")\n",
    "            link.click()\n",
    "        else:\n",
    "            #print(f\"Starting from page {current_page}\")\n",
    "            scroll_to_element_and_click(driver, link)\n",
    "        WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, 'eventRow')))\n",
    "\n",
    "        html = driver.page_source\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        for row in soup.find_all('div', class_='eventRow'):\n",
    "\n",
    "            if row.find('div', class_='text-black-main font-main w-full truncate text-xs font-normal leading-5'):\n",
    "                curr_date = row.find('div', class_='text-black-main font-main w-full truncate text-xs font-normal leading-5').text.strip()\n",
    "\n",
    "            if len(row.find_all('p', attrs={'data-v-a4e7076e': True})) >= 2 and len(row.find_all('p', class_='participant-name truncate')) > 1:\n",
    "                home_team = row.find_all('p', class_='participant-name truncate')[0].text.strip()\n",
    "                away_team = row.find_all('p', class_='participant-name truncate')[1].text.strip()\n",
    "\n",
    "                # Create a unique identifier for each match\n",
    "                match_id = (curr_date, home_team, away_team)\n",
    "                if 'Yesterday' in curr_date or 'Today' in curr_date:\n",
    "                    continue\n",
    "                curr_date_datetime = datetime.strptime(curr_date, \"%d %b %Y\")\n",
    "                if curr_date_datetime < recent_date:\n",
    "                    breakAll = True\n",
    "                    break\n",
    "\n",
    "                if match_id not in unique_matches:\n",
    "                    unique_matches.add(match_id)  # Add the match to the set\n",
    "                    if len(row.find_all('p', attrs={'data-v-34474325': True})) == 3:\n",
    "                        new_row = pd.DataFrame([{\n",
    "                            'season': soup.find('a', 'active-item-calendar').text.strip(),\n",
    "                            'date': curr_date,\n",
    "                            'home_team': home_team,\n",
    "                            'away_team': away_team,\n",
    "                            'h_goals': row.find_all('div', class_='min-mt:!flex')[0].text.strip(),\n",
    "                            'a_goals': row.find_all('div', class_='min-mt:!flex')[1].text.strip(),\n",
    "                            'h_odds': row.find_all('p', attrs={'data-v-34474325': True})[0].text.strip(),\n",
    "                            'd_odds': row.find_all('p', attrs={'data-v-34474325': True})[1].text.strip(),\n",
    "                            'a_odds': row.find_all('p', attrs={'data-v-34474325': True})[2].text.strip()\n",
    "                        }])\n",
    "                    else:\n",
    "                        new_row = pd.DataFrame([{\n",
    "                            'season': soup.find('a', 'active-item-calendar').text.strip(),\n",
    "                            'date': curr_date,\n",
    "                            'home_team': home_team,\n",
    "                            'away_team': away_team,\n",
    "                            'h_goals': row.find_all('div', class_='min-mt:!flex')[0].text.strip(),\n",
    "                            'a_goals': row.find_all('div', class_='min-mt:!flex')[1].text.strip(),\n",
    "                            'h_odds': '100',\n",
    "                            'd_odds': '100',\n",
    "                            'a_odds': '100'\n",
    "                        }])\n",
    "                    new_df = pd.concat([new_df, new_row], ignore_index=True)\n",
    "\n",
    "        if breakAll:\n",
    "            break\n",
    "\n",
    "        previous_page = current_page\n",
    "    \n",
    "    if breakAll:\n",
    "        break\n",
    "\n",
    "driver.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat([new_df, df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Create the SQLAlchemy engine\n",
    "engine = create_engine(connection_string)\n",
    "\n",
    "# Write the DataFrame to a PostgreSQL table\n",
    "try:\n",
    "    # Write the DataFrame to a table named 'your_table_name', replace it with your desired table name\n",
    "    df.to_sql('raw_match_data', engine, index=False, if_exists='replace')\n",
    "\n",
    "    print(\"Table created successfully.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gamePredictor",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
